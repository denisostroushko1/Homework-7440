---
title: "Denis Ostroushko - PUBH 7440 - HW4 - Part 2"
format: pdf
execute: 
  warning: false
  message: false 
  echo: false 
---

```{r local helper, eval= F}
rm(list = ls())
i = 2
```

```{r}
library(tidyverse)
library(kableExtra)
library(gridExtra)

options(scipen = 99999, 
        knitr.kable.NA = '')
```

# Problem 1 

### Prerequisites

In this assignment we analyze stroke-related mortality rates at the county-age-group levels in PA. 

We have 67 counties, $i = 1, 2, \dots, 67$, and three age groups $a=1,2,3$ within each county

We assume that the number of observed death in county $i$ and age group $a$ is distributed by a Poisson distribution with 
parameter $n_{ia} \lambda_{ia}$, where $log \lambda_{ia} = \theta_{0a}$. 

So, the death rate for each county is a function of a random variable with some underlying distribution. 

Given that Poisson distribution parameter is a function of two random variables, we can write pmf of $Y_{ia}$ as: 

\begin{align*}
Y_{ia} &= \frac{e^{- (n_{ia} e^{\theta_{0a}}) } \times (n_{ia} e^{\theta_{0a}})^{Y_{ia}}}{Y_{ia}!}
\end{align*}

As mentioned previously, $\theta_{0a}$ is a random variable {because Bayesian Analysis framework}, and therefore it has a prior distributions: 

$\theta_{ia} | \beta_{0a}, \sigma^2_a$ ~ $N(\beta_{0a}, \sigma^2_a)$. This equation represents 201 distributions for each county. Within an age group, log death rates for the county ar iid normal random variables. 

$\beta_{0a} | \mu = 0, \tau^2_a$ ~ $N(0, \tau^2_a)$, where $\tau^2 = 10,000$. So, the prior distribution of the average log death rate of a given age group is non-informative. 

$\sigma^2_a$~$IG(0.001, 0.001)$, so variance comes from a non-informative Inverse Gamma (IG) distribution. 

**For suppressed values of deaths with county and age-groups levels:** refer to HW4- Part 1 for the imputation scheme. 

### Full hirerachical model 

\begin{align*}
  p(\beta_{0a}, \theta_{ia}, \sigma^2_{0a} | \textbf{Y})   \propto &
  \Pi_{i,a} \left[ Pois(Y_{ia} | n_{ia} * exp(\theta_{0a})) \right]   \times \hspace{1cm} \textcolor{blue}{full \ data \ likelihood} \\
  & I(Y_{ia} > 10) \times \hspace{4cm} \textcolor{blue}{indicator \ of \ observed \ Y_{ia}} \\
  & Norm(\theta_{0a} | \beta_{0a}, \sigma^2_a) \times \hspace{4cm} \textcolor{blue}{prior \ for \ \theta_{0a}} \\
  & Norm(\beta_{0a} | 0, \tau^2_a) \times \hspace{4cm} \textcolor{blue}{prior \ for \ \beta_{0a}} \\ 
  & IG(\sigma^2_a | 0.001, 0.001) \times \hspace{3.5cm} \textcolor{blue}{prior \ for \ \sigma^2_a}
\end{align*}

# Problem 2 

### Full conditional for $\beta_{0a}$

* Term $\beta_{0a}$ appears in the distribution of $\theta_{ia}$ as the mean, and in its own distribution. 
Note, $\beta_{0a}$ is the mean for the age group, and is the average log death rate for a given age group. 
Therefore, we need to consider a joint likelihood function for 67 iid $\theta_{ia}$. 

Full conditional for $\beta_{0a}$, holding $a$ fixed. This full conditional generalizes to three terms for each age group: 

\begin{align*}
p(\beta_{0a} | . ) \propto &  \prod_{i = 1}^{N = 67} (2\pi\sigma^2_a)^{-1} exp(-\frac{1}{2\sigma^2_a} (\theta_{ia} - \beta_{0a})^2) \times \hspace{2cm} \textcolor{blue}{joint \ distribution \ of \theta_{ia}} \\
& (2\pi\tau^2_a)^{-1} exp(-\frac{1}{2 \tau^2} \beta_{0a}^2) \hspace{2cm} \textcolor{blue}{prior \ distribution \ of \beta_{0a}} \\ 
& \propto exp(-\frac{1}{2\sigma^2_a} \sum_{i} (\theta_{ia} - \beta_{0a})^2) \times exp(-\frac{1}{2 \tau^2} \beta_{0a}^2)
\end{align*}

I do not recognize this kernel as a known distribution, so we are not able to use Gibbs sampling technique to obtain posterior distribution of $\beta_{0a}$. We will use Metropolis algorithm to obtain samples for the posterior distribution. More on this later, but we will use Metropolis update because we assume a symmetric candidate density for $\beta_{ia}$

### Full conditional for $\theta_{ia}$

* Term $\theta_{ia}$ appears in the likelihood of the data and its own prior distribution. Other parameters of the hierarchical model  are treated as constants with respect to $\theta_{ia}$. 

This derivation gives a general version of a full conditional distribution for county $i$ and age group $a$, we obtain 201 posterior distributions in our analysis. 

\begin{align*}
p(\theta_{ia} | .) \propto & exp(-n_{ia} - exp(\theta_{ia})) \times (n_{ia}exp(\theta_{ia}))^{Y_{ia}} \times (Y_{ia}!)^{-1} \times \\ 
& (2\pi\sigma^2_a)^{-1} exp (-\frac{1}{2\sigma^2_a} (\theta_{ia} - \beta_{0a})^2) \\ 
& \propto exp(-n_{ia} exp(\theta_{ia}) + \theta_{ia}(Y_{ia} - \frac{1}{2\sigma^2_a}\theta_{ia} + \frac{\beta_{0a}}{\sigma^2_a}))
\end{align*}


I do not recognize this kernel as a known distribution, so we are not able to use Gibbs sampling technique to obtain posterior distribution of $\theta_{ia}$. We will use Metropolis algorithm to obtain samples for the posterior distribution. More on this later, but we will use Metropolis update because we assume a symmetric candidate density for $\theta_{ia}$

### Full conditional for $\sigma^2_{a}$

* Term $\sigma^2_{a}$ appears in the prior distribution of random effects $\theta_{ia}$. All other models terms are treated as constants with respect to $\sigma^2_{a}$, and will make up a normalizing constant for the proper density of the full conditional distribution. 

* Because $\sigma^2_a$ appears in each of 67 $\theta_{ia}$ for a given age group $a$, we need to use a joint likelihood of random effects. Assuming independence of $z_{ia}$, we can obtain join prior distribution as a product of 67 prior densities. 

* Similar to $\beta_{0a}$, we derive a general form of the full conditional distribution for this parameter. In our analysis, we will obtain three posterior distributions for $\sigma^2_a$ for each age group. 

* Inverse Gamma is a conjugate prior to a normal distribution, so we should expect to obtain an Inverse Gamma posterior distribution. We will therefore use Gibbs sampling to get samples of $\sigma^2_a$

Full conditional: 

\begin{align*}
p(\sigma^2_a | . ) & \propto \prod_{i= 1}^{N = 67} \frac{1}{\sqrt{2\pi\sigma^2_a}} exp(-\frac{1}{2\sigma^2_a} \sum_i (\theta_{ia} - \beta_{0a})^2) \times  (\sigma^2_a)^{-\alpha-1} exp(-\beta/\sigma^2_a) \\
& \propto (\sigma^2_a)^{-n/2-a-1} \times exp(-(b + \frac{1}{2}\sum_i (\theta_{ia}-\beta_{0a})^2) * 1/\sigma^2_a)
\end{align*}

Full conditional for $\sigma^2_a$ is proportional to the kernel of the Inverse Gamma with parameters $\alpha = n/2 + a$ and $\beta=$ $\large b + \frac{1}{2} \sum_i (\theta_{ia} - \beta_{0a})^2$

# Problem 3

Code to fit the model and obtain posterior distributions for parameters of interest is attached in the appendix. 

To fit the model, I used the following parameters and candidate densities: 

* Assume a symmetric candidate density $\beta_0$ ~ $Norm(\beta_0^{`}, q)$ where $q=1$

* Assume a symmetric candidate density $\theta_{ia}$ ~ $Norm(\theta_{ia}^{`}, q_{ia})$ where $q_{ia} = 1$ 

**Ratio for Updates** $\beta_{0a}$

Using full conditional for $\beta_{0a}$, and omitting candidate density due to symmetrical property, we obtain the 
following equation to test each proposed sample for acceptance/rejection: 

\begin{align*}
r(\beta^{\star}_{0a}) &= exp(-\frac{1}{2\sigma^2_a}[\sum_{i} (\theta_{ia} - \beta_{0a}^{\star})^2 - \sum_{i}(\theta_{ia} - \beta_{0a}^{`})^2]) \times exp(-\frac{1}{2\tau^2_a} ((\beta_{0a}^{\star})^2 - (\beta_{0a}^{`})^2))
\end{align*}

**Ratio for Updates** $\theta_{ia}$

Using full conditional for $\theta_{ia}$, and omitting candidate density due to symmetrical property, we obtain the 
following equation to test each proposed sample for acceptance/rejection. Recall that each $\theta_{ia}$ is specific to the 
county-age-group level, so we do not aggregate the data, like with the $\beta_{0a}$ example. 

\begin{align*}
r(\theta_{ia}^{\star}) & = exp(-n_{ia}[exp(\theta_{ia}^{\star}) - exp(\theta_{ia}^{`})]) \times exp(Y_{ia}(\theta_{ia}^{\star} - \theta_{ia}^{`})) \times \\ 
& exp(-\frac{1}{2\sigma^2_a}[(\theta_{ia}^{\star} - \beta_{0a})^2 - (\theta_{ia}^{`} - \beta_{0a})^2])
\end{align*}


**Ratios for Update** $\sigma^2_{a}$ is not needed because we use Gibbs sampling 

**Notes for the derivation** are attached at the end of the document after the code section 

```{r data preparation}

stroke=read.table('2016_PA_stroke_total.txt',sep='\t',
                  stringsAsFactors=FALSE,header=TRUE)

stroke_clean <- 
  stroke %>% 
  select(County, Age.Group, Deaths, Population, Crude.Rate) %>% 
  {colnames(.) <- tolower(colnames(.)); 
  . # pass back the data frame with nice column names 
  } %>% 
  rename(crude.rate.100k = crude.rate)

stroke_clean$ratio <- with(stroke_clean, deaths/population)
## let n_0 for prior population size be 1000
n0 = 10000

stroke_clean %>% 
  group_by(age.group) %>% 
  summarise(n_in_group = sum(population)) %>% 
  ungroup() %>% 
  mutate(p_in_group = n_in_group/sum(n_in_group)) %>% 
  select(p_in_group) %>% 
  unlist() -> global_age_proportions


lam0=c(75,250,1000)/100000 ## these are 

stroke_clean$lambda_0 <- rep(lam0, 67)
stroke_clean$p_0 <- rep(global_age_proportions, 67)
stroke_clean$n_0 <- rep(global_age_proportions*n0, 67)

```

```{r M-H sampling, eval = F }

set.seed(1234)

reps = 50000

lambda_ia <- 
  cbind(
    stroke_clean %>% select(lambda_0) %>% unlist(), 
    matrix(data = NA, 
           nrow =   stroke_clean %>% select(lambda_0) %>% unlist() %>% length(), 
           ncol = (reps-1)
             )
  )
## get guesses for beta_oa as the group average from data 
# first, if there are missing values, impute with prior guess for lambda0

stroke_clean %>% 
  mutate(final_y = ifelse(is.na(deaths), lambda_0 * population, deaths), 
         log_rate = log(final_y/population)
         ) %>% 
  group_by(age.group) %>% 
  summarize(b0a = mean(log_rate)) %>% 
  ungroup() %>% 
  select(b0a) %>% 
  unlist() -> boa_guess

beta_0a <- 
  cbind(
    boa_guess, 
    matrix(data = NA, 
           nrow =   3, 
           ncol = (reps-1)
             )
  ) ## these are some pretty bad guesses for the betas, but it will work for now 

## get initial guesses for theta_ia as the log of the prior guess of mortality rate for 
## a corresponding age group 
theta_ai_guess <- log(stroke_clean$lambda_0)

theta_ia <- 
  cbind(
    theta_ai_guess, 
    matrix(data = NA, 
           nrow =   length(theta_ai_guess), 
           ncol = (reps-1)
             )
  ) 

sigma_0a <- 
  cbind(
    c(1,1,1), 
    matrix(data = NA, 
           nrow =   3, 
           ncol = (reps-1)
             )
  )## initial guesses for the variance of theta_ia distribution 

####
# parameters for the prior distributions 
tau2 = 10000  
a = 0.001
b = 0.001 

q_norm_b  = 0.075 # variance of normal candidate densities 
q_norm_theta_ai =1 

n = 67 # number of counties in the analysis 

for(i in 2:reps){
  
  if(i %% 1000 == 0){print(i)}
  ######################
  # DATA IMPUTATION STEP 
  ######################
  
  lambda_ia[,(i-1)] * stroke_clean$population -> poisson_lambdas_iter
  
  ppois(9.5, poisson_lambdas_iter) -> limits_detection_iter 
  
  # using these numbers between 0 and somewhere less than 1, sample from uniform distribution 
  runif(n = length(limits_detection_iter), min = 0, max = limits_detection_iter) -> sampled_u 
  
  # get imputed values by putting unifrom random samples into 'inverse' CDF 
  qpois(sampled_u, lambda = poisson_lambdas_iter) -> imp 
  
  # get final imputed vector of the observed data 
  stroke_clean$deaths -> final_ys_iter
  final_ys_iter[which(is.na(final_ys_iter))] <- imp[which(is.na(final_ys_iter))]
  
  ############
  # UPDATE sigma_a
  
  # sample new sigma from the candidate density 
  ## sample new values of sigma from posterior 
  for(SIGMA in 1:3){

    # s_prop = sig_proposed[SIGMA]
    # s_curr = sigma_0a[,(i-1)][SIGMA]
    # identify what rows of random effects to grab
    theta_ia_rows <- seq(from = SIGMA,
                    to = length(final_ys_iter) - (3- SIGMA),
                    length.out = 67)

    # data for ratio
    theta_ia[theta_ia_rows, (i-1)] -> thetas
    beta <- beta_0a[SIGMA,(i-1)]
    sq_sums <- sum((thetas - beta)^2)

    sigma_0a[,(i)][SIGMA] = 1 / rgamma(n = 1, n/2 + a, sq_sums/2+b )

  }
    
  ############
  # UPDATE B_0a
  for(POP in 1:3){
        
    most_recent_beta0a <- beta_0a[POP, (i-1)] 
    sampled_beta0a <- rnorm(n = 1, mean = most_recent_beta0a, sd = q_norm_b)
    
    POP_rows <- seq(from = POP, 
                    to = length(final_ys_iter) - (3- POP), 
                    length.out = 67)
    
    thetas_ipop = theta_ia[POP_rows, (i-1)]
    
    sum_sq_w_prop = sum((thetas_ipop-sampled_beta0a)^2)
    sum_sq_w_curr = sum((thetas_ipop-most_recent_beta0a)^2)
    
    sigma_calc = sigma_0a[POP, i]
    
    ratio  <- 
      -1/(2 * sigma_calc) * (sum_sq_w_prop - sum_sq_w_curr) - 
      1/(2*tau2) * (sampled_beta0a^2 - most_recent_beta0a^2)
    
    beta_0a[POP, (i)] <- ifelse(exp(ratio) > runif(n=1), sampled_beta0a, most_recent_beta0a)
    
  }
  
  
  ############
  # UPDATE THETA_ia
  
  b_0a = beta_0a[,(i)]
  b_0a_calc = rep(b_0a, n)
  
  sig2 = sigma_0a[,(i)]
  sig2_calc = rep(sig2, n)
  
  theta_ia_curr = theta_ia[,(i-1)]
  
  theta_ia_prop = rnorm(n = n*3, mean = theta_ia_curr, sd = q_norm_theta_ai)
  
  ns = stroke_clean$population
  
  
  ratio <- 
    -ns*(exp(theta_ia_prop) - exp(theta_ia_curr)) +
    final_ys_iter * (theta_ia_prop - theta_ia_curr) - 
    1/(2 * sig2_calc) * (
      (theta_ia_prop - b_0a_calc)^2 - (theta_ia_curr - b_0a_calc)^2
    )
  
  theta_ia[,i] <- ifelse(exp(ratio) > runif(n = length(ratio)), theta_ia_prop, theta_ia_curr)

  
  ## update lambda based on theta
  
  lambda_ia[,(i)] = exp(theta_ia[,i])
  
}

res <- list(lambda_ia, 
                   sigma_0a, 
                   theta_ia, 
                   beta_0a
                   )

names(res) <- c("lambdas", "sigmas", "thetas", "betas")

write_rds(x = res, 
          file = "./MHresults/MH results.rds"
          )
```

<!-- ************************************************
EVALUDATE RESULTS OF METROPOLIS HASTINGS SAMPLER 
************************************************--> 

```{r checking Beta results of M-H version 2, eval = F}

burnin = 1:10000

plotting = beta_0a[1,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1)

mean(plotting)
### 
# comment: 2/20/24 
#   converged, but there are too many rejections 
#   last mean for after burnin:  -111.7797
#   acceptance rate after burnin: 0.6154068 -- too high now 
#   plot over iterations looks amazing tho 


ggplot(data = data.frame(beta0 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = beta0)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1) -> p

### checking other beta_0

#### beta0 a = 2
plotting = beta_0a[2,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1) ## decent

mean(plotting)
summary(plotting)

ggplot(data = data.frame(beta0 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = beta0)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1) -> p

#### beta0 a = 3
# after 25,000 burnin it looks okay 
plotting = beta_0a[3,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))] 
)) / (length(plotting)-1) ## decent 

mean(plotting)

ggplot(data = data.frame(beta0 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = beta0)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1) -> p

```

```{r checking Sigma results of M-H version 2, eval = F}

#         View(sigma_0a)

###############
# SIGMA2 
plotting = sigma_0a[1,][-burnin]

mean(plotting)

ggplot(data = data.frame(sigma20 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = sigma20)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1) -> p

## they look fine but the log plot in the one that really needs to be assessd I think 

##########
plotting = sigma_0a[2,][-burnin]

mean(plotting)

ggplot(data = data.frame(sigma20 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = sigma20)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1) -> p

##########
plotting = sigma_0a[3,][-burnin]

mean(plotting)

ggplot(data = data.frame(sigma20 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = sigma20)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1) -> p
```

<!--
************************************************
PRESENT THE RESULTS
************************************************
-->
### Model Comparison 

```{r analyze results }

results <- readRDS("MHresults/MH results.rds")

dim(results$lambdas)[2] -> length_of_res

burnin = 1:10000

lambdas <- results$lambdas[,-burnin]

final_lambdas <- apply(lambdas, MARGIN = 1, FUN = mean)

stroke_clean %>% 
  group_by(age.group) %>% 
  mutate(age_weight = sum(population)/sum(stroke_clean$population)) %>% 
  ungroup() %>% 
  mutate(model_lambdas = final_lambdas) %>% 
  
  group_by(county) %>% 
  summarise(
    deaths = sum(deaths, na.rm = T), 
    population = sum(population), 
    crude_rate_100k = sum(deaths, na.rm = T) / sum(population) * 100000, 
    age_adj_rate_100k = sum(model_lambdas * age_weight) * 100000
  ) -> rates_data 

aa.med = rates_data$age_adj_rate_100k / 100000

```

```{R for map development purposes, eval = F}

library(sp)
library(maps)
library(RColorBrewer)
rd <- load('penn.rdata')
#         penn

Ns=67 
ncols=7
cols=brewer.pal(ncols,'RdYlBu')[ncols:1]
tcuts=quantile(aa.med*100000,1:(ncols-1)/ncols)
tcolb=array(rep(aa.med*100000,each=ncols-1) > tcuts,
            dim=c(ncols-1,Ns))
tcol =apply(tcolb,2,sum)+1

png('PAmapHW4.png',height=600,width=1000)
par(mar=c(0,0,0,10),cex=1)
    plot(penn,col=cols[tcol],border='lightgray',lwd=.5)
    legend('right',inset=c(-.15,0),xpd=TRUE,
           legend=c(paste(
           c('Below',round(tcuts[-(ncols-1)],0),'Over'),
           c(' ',rep( ' - ',ncols-2),' '),
           c(round(tcuts,0),round(tcuts[ncols-1],0)),sep='')),
           fill=cols,title='Deaths per 100,000',bty='n',cex=1.5,
           border='lightgray')
dev.off()
```

@fig-map  presents county-specific age-adjusted rates using regression with age as a predictor. Overall, the map looks similar to what we observed under the Poisson-Gamma model (HW3). @fig-map-p1 shows results obtained in part 1, 
using mixed effects regression adjustment for age. The two maps appear very similar. 

```{r include final map}
#| label: fig-map 
#| fig-cap: "Final Map of Rates using Age-Adjusted Regression (Part 2)"
#| fig-align: center

knitr::include_graphics('PAmapHW4.png')

```

```{r include final map part 1}
#| label: fig-map-p1 
#| fig-cap: "Final Map of Rates Using Mixed Effects Regression (Part 1)"
#| fig-align: center

knitr::include_graphics(paste0(gsub("Part 2", "Part 1", getwd()), '/PAmapHW4.png'))

```

@fig-lambda-sum presents posterior distribution for the average log-death rate for each group. Top row represents estimates obtained from part 1 , and bottom row are part 2 estimates (this HW). Recall: part 2 is regression for age adjustment, and part 1 is regression for age adjustment with county specific random effects. 

Overall, the two models produce similar posterior distributions for all $\beta_{0a}$. It is quite notable that posterior distributions from part 1 are less smooth. I am not sure why this happens, it could be due to the fact that 
the model is more complex. It also could be due to the fact that my algorithm are not tuned for optimal convergence. 
Perhaps, allowing for 100,000 replications of the loop will smooth out the distributions more. However, it seems that for practical interpretation of the average death rate for the age group, both models produce identical results. 

It seems that there might be minor skewness at the tails of varying degree for all of these posterior distributions. 
Perhaps, tuning sampling procedure to allow distributions to converge at a better and faster rate will result in better posterior distributions. 

```{r import results model 1 }

readRDS(
  paste0(gsub("Part 2", "Part 1", getwd()), "/MHresults/MH results7.rds")
) -> part_1_res

```

```{r betas from part 1 and part 2 comparison }
#| label: fig-beta0-sum-comp
#| fig-cap: "Posterior Distribtuions of Beta0 for the three age groups"
#| fig-width: 14
#| fig-height: 12

## part 2 results 
  results <- readRDS("MHresults/MH results.rds")
  
  dim(results$betas)[2] -> length_of_res
  
  betas <- results$betas[,-burnin]
  
  betas %>% t() %>% data.frame()-> betas_df

## part 1 results 
  dim(part_1_res$betas)[2] -> length_of_res
  
  betas <- part_1_res$betas[,-burnin]
  
  betas %>% t() %>% data.frame()-> betas_df1
  
  
  
grid.arrange(
  ggplot(data = betas_df1, 
         aes(x = b0a1)) + 
    theme_classic() + 
    geom_histogram(bins = 50, color = "black", fill = "lightgrey") + 
    geom_vline(xintercept = mean(betas_df1$b0a1), color = "red", linewidth = 1, linetype = "dashed") + 
    labs(x = "Part 1: Beta0 for Age group of 65-74 years", 
         title = paste0("Posterior Average: ", round(mean(betas_df1$b0a1), 2), 
                        "\nPosterior Std. Deviation: ", round(sd(betas_df1$b0a1), 2), 
                        "\nExponentiated Average: ", round(exp(mean(betas_df1$b0a1)), 4)
                        )
         ),
  
  ggplot(data = betas_df1, 
         aes(x = b0a2)) + 
    theme_classic() + 
    geom_histogram(bins = 50, color = "black", fill = "lightgrey") + 
    geom_vline(xintercept = mean(betas_df1$b0a2), color = "red", linewidth = 1, linetype = "dashed") + 
    labs(x = "Part 1: Beta0 for Age group of 75-84 years", 
         title = paste0("Posterior Average: ", round(mean(betas_df1$b0a2), 2), 
                        "\nPosterior Std. Deviation: ", round(sd(betas_df1$b0a2), 2), 
                        "\nExponentiated Average: ", round(exp(mean(betas_df1$b0a2)), 4)
                        )
         ),
  
  ggplot(data = betas_df1, 
         aes(x = b0a3)) + 
    theme_classic() + 
    geom_histogram(bins = 50, color = "black", fill = "lightgrey") + 
    geom_vline(xintercept = mean(betas_df1$b0a3), color = "red", linewidth = 1, linetype = "dashed") + 
    labs(x = "Part 1: Beta0 for Age group of 85+ years", 
         title = paste0("Posterior Average: ", round(mean(betas_df1$b0a3), 2), 
                        "\nPosterior Std. Deviation: ", round(sd(betas_df1$b0a3), 2), 
                        "\nExponentiated Average: ", round(exp(mean(betas_df1$b0a3)), 4)
                        )
         ), 
  
  ggplot(data = betas_df, 
         aes(x = b0a1)) + 
    theme_classic() + 
    geom_histogram(bins = 50, color = "black", fill = "lightgrey") + 
    geom_vline(xintercept = mean(betas_df$b0a1), color = "red", linewidth = 1, linetype = "dashed") + 
    labs(x = "Part 2: Beta0 for Age group of 65-74 years", 
         title = paste0("Posterior Average: ", round(mean(betas_df$b0a1), 2), 
                        "\nPosterior Std. Deviation: ", round(sd(betas_df$b0a1), 2), 
                        "\nExponentiated Average: ", round(exp(mean(betas_df$b0a1)), 4)
                        )
         ),
  
  ggplot(data = betas_df, 
         aes(x = b0a2)) + 
    theme_classic() + 
    geom_histogram(bins = 50, color = "black", fill = "lightgrey") + 
    geom_vline(xintercept = mean(betas_df$b0a2), color = "red", linewidth = 1, linetype = "dashed") + 
    labs(x = "Part 2: Beta0 for Age group of 75-84 years", 
         title = paste0("Posterior Average: ", round(mean(betas_df$b0a2), 2), 
                        "\nPosterior Std. Deviation: ", round(sd(betas_df$b0a2), 2), 
                        "\nExponentiated Average: ", round(exp(mean(betas_df$b0a2)), 4)
                        )
         ),
  
  ggplot(data = betas_df, 
         aes(x = b0a3)) + 
    theme_classic() + 
    geom_histogram(bins = 50, color = "black", fill = "lightgrey") + 
    geom_vline(xintercept = mean(betas_df$b0a3), color = "red", linewidth = 1, linetype = "dashed") + 
    labs(x = "Part 2: Beta0 for Age group of 85+ years", 
         title = paste0("Posterior Average: ", round(mean(betas_df$b0a3), 2), 
                        "\nPosterior Std. Deviation: ", round(sd(betas_df$b0a3), 2), 
                        "\nExponentiated Average: ", round(exp(mean(betas_df$b0a3)), 4)
                        )
         ), 
  
  nrow = 2, 
  ncol = 3
)

```

Previously, we saw that the average rates of mortality at the county level between method 1 and 2 were quite similar.
@fig-lambda-sum visualizes correlation between method 1 and method 2 estimates for each county. 

The two methods produce very similar estimates. County level estimates are highly correlated, and have a 1-1 correspondence. 

**It appears that the choice of the model does not matter as much, as they both result in identical estimates of stroke related mortality at the county level**

```{r lambdas from part 1 and part 2 comparison }
#| label: fig-lambda-sum
#| fig-cap: "Lambda compariosns"
#| fig-width: 8
#| fig-height: 6

## part 2 results 
  # aa.med

## part 1 results 
  
  dim(part_1_res$lambdas)[2] -> length_of_res
  
  lambdas <- part_1_res$lambdas[,-burnin]
  
  final_lambdas <- apply(lambdas, MARGIN = 1, FUN = mean)
  
  stroke_clean %>% 
    group_by(age.group) %>% 
    mutate(age_weight = sum(population)/sum(stroke_clean$population)) %>% 
    ungroup() %>% 
    mutate(model_lambdas = final_lambdas) %>% 
    
    group_by(county) %>% 
    summarise(
      deaths = sum(deaths, na.rm = T), 
      population = sum(population), 
      crude_rate_100k = sum(deaths, na.rm = T) / sum(population) * 100000, 
      age_adj_rate_100k = sum(model_lambdas * age_weight) * 100000
    ) -> rates_data 
  
  aa.med_p1 = rates_data$age_adj_rate_100k / 100000
  
min_1 = min(aa.med)
min_2 = min(aa.med_p1)
all_min = min(min_1, min_2)
  
max_1 = min(aa.med)
max_2 = min(aa.med_p1)
all_max = min(max_1, max_2)
  
ggplot(data = data.frame(part_1 = aa.med_p1, 
                         part_2 = aa.med, 
                         pop = stroke_clean$population), 
       aes(x = part_1, 
           y = part_2,
           color = pop)) + 
  theme_classic() + 
  geom_point() + 
  scale_color_gradientn(colors = rainbow(5)) + 
  labs(x = "Lambda Rates from Part 1 of HW4", 
       y = "Lambda Rates from Part 2 of HW4",
       color = "County \nPopulation", 
       title = paste0("Correlation of rates from the two models: ", 
                 round(cor(aa.med, aa.med_p1), 4), 
                 "\nRegression Coefficient: ", 
                 round(summary(lm(aa.med~aa.med_p1))$coefficients[2,1], 4))) + 
  geom_abline(slope = 1, color = "black", alpha = 0.5, linetype = "dashed")

```

\newpage 

# Appendix 

### Metropolis Hastings Code 

```{r M-H sampling viz, eval = F, echo = T }

set.seed(1234)

reps = 50000

lambda_ia <- 
  cbind(
    stroke_clean %>% select(lambda_0) %>% unlist(), 
    matrix(data = NA, 
           nrow =   stroke_clean %>% select(lambda_0) %>% unlist() %>% length(), 
           ncol = (reps-1)
             )
  )
## get guesses for beta_oa as the group average from data 
# first, if there are missing values, impute with prior guess for lambda0

stroke_clean %>% 
  mutate(final_y = ifelse(is.na(deaths), lambda_0 * population, deaths), 
         log_rate = log(final_y/population)
         ) %>% 
  group_by(age.group) %>% 
  summarize(b0a = mean(log_rate)) %>% 
  ungroup() %>% 
  select(b0a) %>% 
  unlist() -> boa_guess

beta_0a <- 
  cbind(
    boa_guess, 
    matrix(data = NA, 
           nrow =   3, 
           ncol = (reps-1)
             )
  ) ## these are some pretty bad guesses for the betas, but it will work for now 

## get initial guesses for theta_ia as the log of the prior guess of mortality rate for 
## a corresponding age group 
theta_ai_guess <- log(stroke_clean$lambda_0)

theta_ia <- 
  cbind(
    theta_ai_guess, 
    matrix(data = NA, 
           nrow =   length(theta_ai_guess), 
           ncol = (reps-1)
             )
  ) 

sigma_0a <- 
  cbind(
    c(1,1,1), 
    matrix(data = NA, 
           nrow =   3, 
           ncol = (reps-1)
             )
  )## initial guesses for the variance of theta_ia distribution 

####
# parameters for the prior distributions 
tau2 = 10000  
a = 0.001
b = 0.001 

q_norm_b  = 0.075 # variance of normal candidate densities 
q_norm_theta_ai =1 

n = 67 # number of counties in the analysis 

for(i in 2:reps){
  
  if(i %% 1000 == 0){print(i)}
  ######################
  # DATA IMPUTATION STEP 
  ######################
  
  lambda_ia[,(i-1)] * stroke_clean$population -> poisson_lambdas_iter
  
  ppois(9.5, poisson_lambdas_iter) -> limits_detection_iter 
  
  # using these numbers between 0 and somewhere less than 1, sample from uniform distribution 
  runif(n = length(limits_detection_iter), min = 0, max = limits_detection_iter) -> sampled_u 
  
  # get imputed values by putting unifrom random samples into 'inverse' CDF 
  qpois(sampled_u, lambda = poisson_lambdas_iter) -> imp 
  
  # get final imputed vector of the observed data 
  stroke_clean$deaths -> final_ys_iter
  final_ys_iter[which(is.na(final_ys_iter))] <- imp[which(is.na(final_ys_iter))]
  
  ############
  # UPDATE sigma_a
  
  # sample new sigma from the candidate density 
  ## sample new values of sigma from posterior 
  for(SIGMA in 1:3){

    # s_prop = sig_proposed[SIGMA]
    # s_curr = sigma_0a[,(i-1)][SIGMA]
    # identify what rows of random effects to grab
    theta_ia_rows <- seq(from = SIGMA,
                    to = length(final_ys_iter) - (3- SIGMA),
                    length.out = 67)

    # data for ratio
    theta_ia[theta_ia_rows, (i-1)] -> thetas
    beta <- beta_0a[SIGMA,(i-1)]
    sq_sums <- sum((thetas - beta)^2)

    sigma_0a[,(i)][SIGMA] = 1 / rgamma(n = 1, n/2 + a, sq_sums/2+b )

  }
    
  ############
  # UPDATE B_0a
  for(POP in 1:3){
        
    most_recent_beta0a <- beta_0a[POP, (i-1)] 
    sampled_beta0a <- rnorm(n = 1, mean = most_recent_beta0a, sd = q_norm_b)
    
    POP_rows <- seq(from = POP, 
                    to = length(final_ys_iter) - (3- POP), 
                    length.out = 67)
    
    thetas_ipop = theta_ia[POP_rows, (i-1)]
    
    sum_sq_w_prop = sum((thetas_ipop-sampled_beta0a)^2)
    sum_sq_w_curr = sum((thetas_ipop-most_recent_beta0a)^2)
    
    sigma_calc = sigma_0a[POP, i]
    
    ratio  <- 
      -1/(2 * sigma_calc) * (sum_sq_w_prop - sum_sq_w_curr) - 
      1/(2*tau2) * (sampled_beta0a^2 - most_recent_beta0a^2)
    
    beta_0a[POP, (i)] <- ifelse(exp(ratio) > runif(n=1), sampled_beta0a, most_recent_beta0a)
    
  }
  
  
  ############
  # UPDATE THETA_ia
  
  b_0a = beta_0a[,(i)]
  b_0a_calc = rep(b_0a, n)
  
  sig2 = sigma_0a[,(i)]
  sig2_calc = rep(sig2, n)
  
  theta_ia_curr = theta_ia[,(i-1)]
  
  theta_ia_prop = rnorm(n = n*3, mean = theta_ia_curr, sd = q_norm_theta_ai)
  
  ns = stroke_clean$population
  
  
  ratio <- 
    -ns*(exp(theta_ia_prop) - exp(theta_ia_curr)) +
    final_ys_iter * (theta_ia_prop - theta_ia_curr) - 
    1/(2 * sig2_calc) * (
      (theta_ia_prop - b_0a_calc)^2 - (theta_ia_curr - b_0a_calc)^2
    )
  
  theta_ia[,i] <- ifelse(exp(ratio) > runif(n = length(ratio)), theta_ia_prop, theta_ia_curr)

  
  ## update lambda based on theta
  
  lambda_ia[,(i)] = exp(theta_ia[,i])
  
}

res <- list(lambda_ia, 
                   sigma_0a, 
                   theta_ia, 
                   beta_0a
                   )

names(res) <- c("lambdas", "sigmas", "thetas", "betas")

write_rds(x = res, 
          file = "./MHresults/MH results.rds"
          )
```