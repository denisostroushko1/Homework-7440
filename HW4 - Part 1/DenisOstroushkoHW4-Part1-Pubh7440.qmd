---
title: "Denis Ostroushko - PUBH 7440 - HW4 - Part 1"
format: pdf
execute: 
  warning: false
  message: false 
  echo: false 
---

```{r}
library(tidyverse)
library(kableExtra)

options(scipen = 99999, 
        knitr.kable.NA = '')
```

```{r data preparation}

stroke=read.table('2016_PA_stroke_total.txt',sep='\t',
                  stringsAsFactors=FALSE,header=TRUE)

stroke_clean <- 
  stroke %>% 
  select(County, Age.Group, Deaths, Population, Crude.Rate) %>% 
  {colnames(.) <- tolower(colnames(.)); 
  . # pass back the data frame with nice column names 
  } %>% 
  rename(crude.rate.100k = crude.rate)

stroke_clean$ratio <- with(stroke_clean, deaths/population)
## let n_0 for prior population size be 1000
n0 = 10000

stroke_clean %>% 
  group_by(age.group) %>% 
  summarise(n_in_group = sum(population)) %>% 
  ungroup() %>% 
  mutate(p_in_group = n_in_group/sum(n_in_group)) %>% 
  select(p_in_group) %>% 
  unlist() -> global_age_proportions


lam0=c(75,250,1000)/100000 ## these are 

stroke_clean$lambda_0 <- rep(lam0, 67)
stroke_clean$p_0 <- rep(global_age_proportions, 67)
stroke_clean$n_0 <- rep(global_age_proportions*n0, 67)

```


```{r M-H sampling, eval = F }

set.seed(182)

reps = 100000

lambda_ia <- 
  cbind(
    stroke_clean %>% select(lambda_0) %>% unlist(), 
    matrix(data = NA, 
           nrow =   stroke_clean %>% select(lambda_0) %>% unlist() %>% length(), 
           ncol = (reps-1)
             )
  )
## get guesses for beta_oa as the group average from data 
# first, if there are missing values, impute with prior guess for lambda0

stroke_clean %>% 
  mutate(final_y = ifelse(is.na(deaths), lambda_0 * population, deaths), 
         log_rate = log(final_y/population)
         ) %>% 
  group_by(age.group) %>% 
  summarize(b0a = mean(log_rate)) %>% 
  ungroup() %>% 
  select(b0a) %>% 
  unlist() -> boa_guess

beta_0a <- 
  cbind(
    boa_guess, 
    matrix(data = NA, 
           nrow =   3, 
           ncol = (reps-1)
             )
  ) ## these are some pretty bad guesses for the betas, but it will work for now 

## get initial guesses for z_ia as the difference between observed Y minus age_group average 
# first, if there are missing values, impute with prior guess for lambda0

stroke_clean %>% 
  mutate(final_y = ifelse(is.na(deaths), lambda_0 * population, deaths), 
         log_rate = log(final_y/population)
         ) %>% 
  group_by(age.group) %>% 
  mutate(b0a = mean(log_rate)) %>% 
  ungroup() %>% 
  mutate(z_ia = log_rate -b0a) %>%
  select(z_ia) %>% 
  unlist() -> zi_guess

z_ia <- 
  cbind(
    zi_guess, 
    matrix(data = NA, 
           nrow =   length(zi_guess), 
           ncol = (reps-1)
             )
  ) ## these are some pretty bad guesses for the z_i's, but it will work for now 

sigma_0a <- 
  cbind(
    c(1,1,1), 
    matrix(data = NA, 
           nrow =   3, 
           ncol = (reps-1)
             )
  )## these are some pretty bad guesses for the z_i's, but it will work for now 

tau2 = 10000 
a = 0.001
b = 0.001 

q_norm_b  = 0.05
# q_norm_zi = 0.05
q_norm_zi = abs(zi_guess) # make it such that the step size for each z_ia is poportional to its data-driven estiamte
#                             ratio is 1/1
q_ig = 1

n = 67

for(i in 2:reps){
  
  if(i %% 1000 == 0){print(i)}
  ######################
  # DATA IMPUTATION STEP 
  ######################
  
  lambda_ia[,(i-1)] * stroke_clean$population -> poisson_lambdas_iter
  
  ppois(9.5, poisson_lambdas_iter) -> limits_detection_iter 
  
  # using these numbers between 0 and somewhere less than 1, sample from uniform distribution 
  runif(n = length(limits_detection_iter), min = 0, max = limits_detection_iter) -> sampled_u 
  
  # get imputed values by putting unifrom random samples into 'inverse' CDF 
  qpois(sampled_u, lambda = poisson_lambdas_iter) -> imp 
  
  # get final imputed vector of the observed data 
  stroke_clean$deaths -> final_ys_iter
  final_ys_iter[which(is.na(final_ys_iter))] <- imp[which(is.na(final_ys_iter))]
  
  ############
  # UPDATE sigma_a
  
  # sample new sigma from the candidate density 
  sig_proposed = 1/rgamma(n = 3, q_ig, q_ig * sigma_0a[,(i-1)]) # values 1,2,3 correspond to young, mid, old age groups 
  
  for(SIGMA in 1:3){
    
    s_prop = sig_proposed[SIGMA]
    s_curr = sigma_0a[,(i-1)][SIGMA]
    # identify what rows of random effects to grab 
    z_rows <- seq(from = SIGMA, 
                    to = length(final_ys_iter) - (3- SIGMA), 
                    length.out = 67)
    
    # data for ratio 
    z_ia[z_rows, (i-1)] -> random_effs
    
    (s_prop/s_curr)^(2*q_ig - a - n/2) * 
      
      exp(-1/2 * sum(random_effs^2) * (1/s_prop - 1/s_curr)) * 
      
      exp(-b * ((1/s_prop - 1/s_curr))) * 
      
      exp(q_ig * (s_prop/s_curr - s_curr/s_prop)) -> ratio 
    
    sigma_0a[,(i)][SIGMA] <- ifelse(ratio > runif(1), s_prop, s_curr)
    
  }
  
  ############
  # UPDATE Z_ia
  
  b_0a = beta_0a[,(i-1)]
  b_0a_calc = rep(b_0a, n)
  
  sig2 = sigma_0a[,(i)]
  sig2_calc = rep(sig2, n)
  
  z_ia_curr = z_ia[,(i-1)]
  z_ia_prop = rnorm(n = n*3, mean = z_ia_curr, sd = q_norm_zi)
  
  (-stroke_clean$population * 
        (exp(b_0a_calc + z_ia_prop) + exp(b_0a_calc +z_ia_curr ))
      ) + 
    
    (final_ys_iter*(z_ia_prop - z_ia_curr)) + 
    
    (-1/(2 * sig2_calc) * (z_ia_prop^2 - z_ia_curr^2)) -> ratio 
  
  z_ia[,i] <- ifelse(exp(ratio) > runif(n = length(ratio)), z_ia_prop, z_ia_curr)
  ############
  # UPDATE B_0a
  
  for(POP in 1:3){
    
    most_recent_beta0a <- beta_0a[POP, (i-1)] 
    sampled_beta0a <- rnorm(n = 1, mean = most_recent_beta0a, sd = q_norm_b)
    
    POP_rows <- seq(from = POP, 
                    to = length(final_ys_iter) - (3- POP), 
                    length.out = 67)
    
    Y_ipop = final_ys_iter[POP_rows]
    n_ipo = stroke_clean$population[POP_rows]
    z_ia_calc = z_ia[,i][POP_rows]
    
    ratio  <- 
      (sum(Y_ipop) * (sampled_beta0a - most_recent_beta0a)) + 
      
      (sum(n_ipo * exp(z_ia_calc)) * (exp(most_recent_beta0a) - exp(sampled_beta0a)) ) + 
      
      (-1/(2 * tau2) * (sampled_beta0a^2 - most_recent_beta0a^2))
    
    beta_0a[POP, (i)] <- ifelse(exp(ratio) > runif(n=1), sampled_beta0a, most_recent_beta0a) 
  }
  
  ## update lambda based on beta and zeta 
  beta_0a_for_lambda = rep(beta_0a[,i], n)
  
  lambda_ia[,(i)] = exp(beta_0a_for_lambda + z_ia[,i])
  
}

res <- list(lambda_ia, 
                   sigma_0a, 
                   z_ia, 
                   beta_0a
                   )

names(res) <- c("lambdas", "sigmas", "zs", "betas")

write_rds(x = res, 
          file = "MH results2.rds"
          )
```

```{r checking results of M-H version 1, eval = F}

View(beta_0a)
View(z_ia) 
View(sigma_0a)
View(lambda_ia)

      
      ########
      # BETA 0 a = 1

burnin = 1:1000

plotting = beta_0a[1,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1)

### 
# comment: 2/20/24 
#   converged, but there are too many rejections 
#   last mean for after burnin:  -7.19
#   acceptance rate after burnin: 0.08056451 -- too low 

ggplot(data = data.frame(beta0 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = beta0)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1)

### checking other beta_0

#### beta0 a = 2
plotting = beta_0a[2,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1)

#### beta0 a = 3
plotting = beta_0a[3,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1)

###############
# SIGMA2 
plotting = sigma_0a[1,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1)

mean(plotting)

### 
# comment: 2/20/24 
#   did not converge, also there are too many rejections 
#   last mean for after burnin:  1.59593
#   acceptance rate after burnin: 0.2466941 -- too low 

ggplot(data = data.frame(sigma20 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = sigma20)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1) + 
  ylim(0, 2)

### checking other beta_0

#### beta0 a = 2
plotting = sigma_0a[2,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1)

#### beta0 a = 3
plotting = sigma_0a[3,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1)

############
# Z_ia 

plotting = z_ia[1,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1)

mean(plotting)

### 
# comment: 2/20/24 
#   did not update at all, 

summary_z_ia = z_ia[,-burnin]

## some updated, but not good at all 
length_unique = function(x){length(unique(x))}
apply(summary_z_ia, 1, length_unique) %>% 
  unique()

```

```{r checking results of M-H version 2, eval = F}

View(beta_0a)
View(z_ia) 
View(sigma_0a)
View(lambda_ia)

      
# new parameters 
# reps = 100000
# tau2 = 10000 
# a = 0.001
# b = 0.001 
# 
# q_norm = .05
# q_ig = 1
# 
# n = 67
# 
# i = 2

      ########
      # BETA 0 a = 1

burnin = 1:10000

plotting = beta_0a[1,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1)

mean(plotting)
### 
# comment: 2/20/24 
#   converged, but there are too many rejections 
#   last mean for after burnin:  -111.7797
#   acceptance rate after burnin: 0.6154068 -- too high now 
#   plot over iterations looks amazing tho 


ggplot(data = data.frame(beta0 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = beta0)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1) -> p

### checking other beta_0

#### beta0 a = 2
plotting = beta_0a[2,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1) ## decent

mean(plotting)

ggplot(data = data.frame(beta0 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = beta0)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1) -> p

#### beta0 a = 3
# after 25,000 burnin it looks okay 
plotting = beta_0a[3,][-(1:25000)]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))] 
)) / (length(plotting)-1) ## decent 

mean(plotting)

ggplot(data = data.frame(beta0 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = beta0)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1) -> p

###############
# SIGMA2 
plotting = sigma_0a[1,][-burnin]
plotting = log(plotting)

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1) ## too low 

mean(plotting)

ggplot(data = data.frame(sigma20 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = sigma20)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1) -> p

## they look fine but the log plot in the one that really needs to be assessd I think 

############
# Z_ia 

plotting = z_ia[1,][-burnin]

length(which(
  plotting[1:(length(plotting)-1)] != 
  plotting[2:(length(plotting))]
)) / (length(plotting)-1) ## all updated all the time 

mean(plotting) 

ggplot(data = data.frame(sigma20 = plotting, 
                         n = 1:length(plotting)), 
       aes(x = n, 
           y = sigma20)) + 
  theme_classic() + 
  geom_line() + 
  geom_hline(yintercept = mean(plotting), color = "red", linewidth = 1) -> p
## plot looks good, but the acceptance rate is like 1, so all the time 



### 
# comment: 2/20/24 
#   did not update at all, 

summary_z_ia = z_ia[,-burnin]

## some updated, but not good at all 
length_unique = function(x){length(unique(x))}
apply(summary_z_ia, 1, length_unique) /reps -> rates 

View(data.frame(rates))
summary(rates)

hist(rates)

```

```{r analyze results }

results <- readRDS("MH results2.rds")

lambdas <- results$lambdas[,-(1:25000)]

final_lambdas <- apply(lambdas, MARGIN = 1, FUN = mean)

stroke_clean %>% 
  group_by(age.group) %>% 
  mutate(age_weight = sum(population)/sum(stroke_clean$population)) %>% 
  ungroup() %>% 
  mutate(model_lambdas = final_lambdas) %>% 
  
  group_by(county) %>% 
  summarise(
    deaths = sum(deaths), 
    population = sum(population), 
    crude_rate_100k = sum(deaths) / sum(population) * 100000, 
    age_adj_rate_100k = sum(model_lambdas * age_weight) * 100000
  ) -> rates_data 

aa.med = rates_data$age_adj_rate_100k / 100000




```

```{R for development purposes, eval = F}

library(sp)
library(maps)
library(RColorBrewer)
rd <- load('penn.rdata')
#         penn

Ns=67 
ncols=7
cols=brewer.pal(ncols,'RdYlBu')[ncols:1]
tcuts=quantile(aa.med*100000,1:(ncols-1)/ncols)
tcolb=array(rep(aa.med*100000,each=ncols-1) > tcuts,
            dim=c(ncols-1,Ns))
tcol =apply(tcolb,2,sum)+1

png('PAmap.png',height=1200,width=1400)
par(mar=c(0,0,0,10),cex=1)
    plot(penn,col=cols[tcol],border='lightgray',lwd=.5)
    legend('right',inset=c(-.15,0),xpd=TRUE,
           legend=c(paste(
           c('Below',round(tcuts[-(ncols-1)],0),'Over'),
           c(' ',rep( ' - ',ncols-2),' '),
           c(round(tcuts,0),round(tcuts[ncols-1],0)),sep='')),
           fill=cols,title='Deaths per 100,000',bty='n',cex=1.5,
           border='lightgray')
dev.off()
```

@fig-map  is the resulting map

```{r include final map}
#| label: fig-map 
#| fig-cap: "Final Map of Rates"
#| fig-align: left
#| fig-width: 11
#| fig-height: 8


knitr::include_graphics('PAmap.png')

```

